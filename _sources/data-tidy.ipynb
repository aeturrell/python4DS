{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f0a171",
   "metadata": {},
   "source": [
    "(data-tidy)=\n",
    "# Tidy Data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this chapter, you will learn a consistent way to organise your data in Python using the principle known as *tidy data*. Tidy data is not appropriate for everything, but for a lot of analysis and a lot of tabular data it will be what you need. Getting your data into this format requires some work up front, but that work pays off in the long term. Once you have tidy data, you will spend much less time munging data from one representation to another, allowing you to spend more time on the data questions you care about.\n",
    "\n",
    "In this chapter, you'll first learn the definition of tidy data and see it applied to simple toy dataset. Then we'll dive into the main tool you'll use for tidying data: melting. Melting allows you to change the form of your data, without changing any of the values. We'll finish up with a discussion of usefully untidy data, and how you can create it if needed.\n",
    "\n",
    "If you particularly enjoy this chapter and want to learn more about the underlying theory, you can learn more in the [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) paper published in the Journal of Statistical Software.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55374",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# remove cell\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use(\"https://github.com/aeturrell/python4DS/raw/main/plot_style.txt\")\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17575f3a",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "This chapter will use the **pandas** data analysis package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f8255",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "There are three interrelated features that make a dataset tidy:\n",
    "\n",
    "1.  Each variable is a column; each column is a variable.\n",
    "2.  Each observation is row; each row is an observation.\n",
    "3.  Each value is a cell; each cell is a single value.\n",
    "\n",
    "The figure below shows this:\n",
    "\n",
    "![](https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png)\n",
    "\n",
    "Why ensure that your data is tidy? There are two main advantages:\n",
    "\n",
    "1.  There's a general advantage to picking one consistent way of storing data.\n",
    "    If you have a consistent data structure, it's easier to learn the tools that work with it because they have an underlying uniformity. Some tools, for example data visualisation package **seaborn**, are designed with tidy data in mind.\n",
    "\n",
    "2.  There's a specific advantage to placing variables in columns because it allows you to take advantage of **pandas**' vectorised operations (operations that are more efficient).\n",
    "\n",
    "\n",
    "Tidy data aren't going to be appropriate *every* time and in every case, but they're a really, really good default for tabular data. Once you use it as your default, it's easier to think about how to perform subsequent operations.\n",
    "\n",
    "Having said that tidy data are great, they are, but one of **pandas**' advantages relative to other data analysis libraries is that it isn't *too* tied to tidy data and can navigate awkward non-tidy data manipulation tasks happily too.\n",
    "\n",
    "There are two common problems you find in data that are ingested that make them not tidy:\n",
    "\n",
    "1. A variable might be spread across multiple columns.\n",
    "2. An observation might be scattered across multiple rows.\n",
    "\n",
    "For the former, we need to \"melt\" the wide data, with multiple columns, into long data.\n",
    "\n",
    "For the latter, we need to unstack or pivot the multiple rows into columns (ie go from long to wide.)\n",
    "\n",
    "We'll see both below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8cf13",
   "metadata": {},
   "source": [
    "## Tools to Make Data Tidy with **pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd134b04",
   "metadata": {},
   "source": [
    "### Melt\n",
    "\n",
    "`melt()` can help you go from \"wider\" data to \"longer\" data, and is a *really* good one to remember.\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/reshaping_melt.png)\n",
    "\n",
    "Here's an example of it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"first\": [\"John\", \"Mary\"],\n",
    "        \"last\": [\"Doe\", \"Bo\"],\n",
    "        \"job\": [\"Nurse\", \"Economist\"],\n",
    "        \"height\": [5.5, 6.0],\n",
    "        \"weight\": [130, 150],\n",
    "    }\n",
    ")\n",
    "print(\"\\n Unmelted: \")\n",
    "print(df)\n",
    "print(\"\\n Melted: \")\n",
    "df.melt(id_vars=[\"first\", \"last\"], var_name=\"quantity\", value_vars=[\"height\", \"weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62978fd",
   "metadata": {},
   "source": [
    "```{admonition} Exercise\n",
    "Perform a `melt()` that uses `job` as the id instead of `first` and `last`.\n",
    "```\n",
    "\n",
    "How does this relate to tidy data? Sometimes you'll have a variable spread over multiple columns that you want to turn tidy. Let's look at this example that uses cases of [tuburculosis from the World Health Organisation](https://www.who.int/teams/global-tuberculosis-programme/data).\n",
    "\n",
    "First let's open the data and look at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa121cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb = pd.read_parquet(\n",
    "    \"https://github.com/aeturrell/python4DS/raw/refs/heads/main/data/who_tb_cases.parquet\"\n",
    ")\n",
    "df_tb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583e419d",
   "metadata": {},
   "source": [
    "You can see that we have two columns for a single variable, year. Let's now melt this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc03ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb.melt(\n",
    "    id_vars=[\"country\"],\n",
    "    var_name=\"year\",\n",
    "    value_vars=[\"1999\", \"2000\"],\n",
    "    value_name=\"cases\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d81b30",
   "metadata": {},
   "source": [
    "We now have one observation per row, and one variable per column: tidy!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f5f10",
   "metadata": {},
   "source": [
    "### A simpler wide to long\n",
    "\n",
    "If you don't want the headscratching of `melt()`, there's also `wide_to_long()`, which is really useful for typical data cleaning cases where you have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293768c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"A1970\": {0: \"a\", 1: \"b\", 2: \"c\"},\n",
    "        \"A1980\": {0: \"d\", 1: \"e\", 2: \"f\"},\n",
    "        \"B1970\": {0: 2.5, 1: 1.2, 2: 0.7},\n",
    "        \"B1980\": {0: 3.2, 1: 1.3, 2: 0.1},\n",
    "        \"X\": dict(zip(range(3), np.random.randn(3))),\n",
    "        \"id\": dict(zip(range(3), range(3))),\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c09e6",
   "metadata": {},
   "source": [
    "i.e. data where there are different variables and time periods across the columns. Wide to long is going to let us give info on what the stubnames are ('A', 'B'), the name of the variable that's always across columns (here, a year), any values (X here), and an id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca2fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.wide_to_long(df, stubnames=[\"A\", \"B\"], i=\"id\", j=\"year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6bf17c",
   "metadata": {},
   "source": [
    "### Stack and Unstack\n",
    "\n",
    "Stack, `stack()` is a shortcut for taking a single type of wide data variable from columns and turning it into a long form dataset, but with an extra index.\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/reshaping_stack.png)\n",
    "\n",
    "Unstack, `unstack()` unsurprisingly does the same operation, but in reverse.\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/reshaping_unstack.png)\n",
    "\n",
    "Let's define a multi-index data frame to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b791dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = list(\n",
    "    zip(\n",
    "        *[\n",
    "            [\"bar\", \"bar\", \"baz\", \"baz\", \"foo\", \"foo\", \"qux\", \"qux\"],\n",
    "            [\"one\", \"two\", \"one\", \"two\", \"one\", \"two\", \"one\", \"two\"],\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n",
    "df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[\"A\", \"B\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7c349",
   "metadata": {},
   "source": [
    "Let's stack this to create a tidy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25eb012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.stack()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeef123",
   "metadata": {},
   "source": [
    "This has automatically created a multi-layered index but that can be reverted to a numbered index using `df.reset_index()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e4e03",
   "metadata": {},
   "source": [
    "Now let's see unstack but, instead of unstacking the 'A', 'B' variables we began with, let's unstack the 'first' column by passing `level=0` (the default is to unstack the innermost index). This diagram shows what's going on:\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/reshaping_unstack_0.png)\n",
    "\n",
    "And here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6742a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185eae1f",
   "metadata": {},
   "source": [
    "```{admonition} Exercise\n",
    "What happens if you unstack to `level=1` instead? What about applying `unstack()` twice?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a210bb",
   "metadata": {},
   "source": [
    "### Pivoting data from long to wide\n",
    "\n",
    "`pivot()` and `pivot_table()` help you to sort out data in which a single observation is scattered over multiple rows.\n",
    "\n",
    "![](https://pandas.pydata.org/docs/_images/reshaping_pivot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbcedf",
   "metadata": {},
   "source": [
    "Here's an example dataframe where observations are spread over multiple rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa612456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tb_cp = pd.read_parquet(\n",
    "    \"https://github.com/aeturrell/python4DS/raw/refs/heads/main/data/who_tb_case_and_pop.parquet\"\n",
    ")\n",
    "df_tb_cp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a4077",
   "metadata": {},
   "source": [
    "You see that we have, for each year-country, \"case\" and \"population\" in different rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9ed1b",
   "metadata": {},
   "source": [
    "Now let's pivot this to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = df_tb_cp.pivot(\n",
    "    index=[\"country\", \"year\"], columns=[\"type\"], values=\"count\"\n",
    ").reset_index()\n",
    "pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82307c0",
   "metadata": {},
   "source": [
    "Pivots are especially useful for time series data, where operations like `shift()` or `diff()` are typically applied assuming that an entry in one row follows (in time) from the one above. When we do `shift()` we often want to shift a single variable in time, but if a single observation (in this case a date) is over multiple rows, the timing is going go awry. Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    \"value\": np.random.randn(20),\n",
    "    \"variable\": [\"A\"] * 10 + [\"B\"] * 10,\n",
    "    \"date\": (\n",
    "        list(pd.date_range(\"1/1/2000\", periods=10, freq=\"ME\"))\n",
    "        + list(pd.date_range(\"1/1/2000\", periods=10, freq=\"ME\"))\n",
    "    ),\n",
    "}\n",
    "df = pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a5b930",
   "metadata": {},
   "source": [
    "If we just run `shift()` on the above, it's going to shift variable B's and A's together even though these overlap in time and are different variables. So we pivot to a wider format (and then we can shift in time safely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f2bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot(index=\"date\", columns=\"variable\", values=\"value\").shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c4821",
   "metadata": {},
   "source": [
    "```{admonition} Exercise\n",
    "Why is the first entry NaN?\n",
    "```\n",
    "\n",
    "\n",
    "```{admonition} Exercise\n",
    "Perform a `pivot()` that applies to both the `variable` and `category` columns in the example from above where category is defined such that `df[\"category\"] = np.random.choice([\"type1\", \"type2\", \"type3\", \"type4\"], 20). (Hint: remember that you will need to pass multiple objects via a list.)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
